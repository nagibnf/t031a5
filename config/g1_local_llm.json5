// Configuração de LLM Local - Sistema t031a5
// Para Tobias (G1) - Jetson Orin NX 16GB
// Llama-3.1-8B-Instruct + OpenAI GPT-4o fallback

{
  "name": "G1 Local LLM Configuration",
  "description": "Configuração de LLM local para Tobias (G1) - Jetson Orin NX 16GB",
  "version": "1.0",
  "llm": {
    "primary": {
      "provider": "llama_cpp",
      "model": "llama-3.1-8b-instruct-q4_k_m.gguf",
      "model_path": "models/llama-3.1-8b-instruct-q4_k_m.gguf",
      "context_length": 4096,
      "temperature": 0.7,
      "max_tokens": 512,
      "top_p": 0.9,
      "frequency_penalty": 0.0,
      "presence_penalty": 0.0,
      "threads": 8,
      "gpu_layers": 32,
      "batch_size": 512,
      "use_mmap": true,
      "use_mlock": false,
      "n_ctx": 4096,
      "n_batch": 512,
      "n_gpu_layers": 32,
      "rope_scaling": {
        "type": "linear",
        "factor": 1.0
      }
    },
    "fallback": {
      "provider": "openai",
      "model": "gpt-4o-mini",
      "api_key": "${OPENAI_API_KEY}",
      "temperature": 0.7,
      "max_tokens": 512
    },
    "routing": {
      "online": "openai",
      "offline": "llama_cpp",
      "fallback": "llama_cpp",
      "auto_switch": true
    }
  },
  "performance": {
    "jetson": {
      "model": "orin_nx_16gb",
      "ram": "16GB",
      "gpu": "1024-core_ampere",
      "tensor_cores": 32,
      "optimization": "max"
    },
    "expected": {
      "response_time": "2-5 seconds",
      "memory_usage": "4-6GB",
      "gpu_usage": "2-4GB",
      "throughput": "10-20 tokens/second"
    }
  }
}
