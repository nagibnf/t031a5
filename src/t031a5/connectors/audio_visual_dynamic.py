# -*- coding: utf-8 -*-
"""
AudioVisualDynamic - Sistema completo √°udio-visual din√¢mico
Combina: ElevenLabs TTS + Anker + G1 LEDs + an√°lise volume em tempo real
"""

import logging
import asyncio
import threading
import time
import wave
import struct
import numpy as np
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class EmotionConfig:
    """Configura√ß√£o de emo√ß√£o para LEDs"""
    name: str
    rgb: Tuple[int, int, int]
    base_intensity: float  # Intensidade base (0.0-1.0)
    keywords: list  # Palavras-chave que ativam esta emo√ß√£o

# Mapeamento completo de emo√ß√µes
EMOTION_CONFIGS = {
    "happy": EmotionConfig("happy", (0, 255, 0), 0.8, 
                          ["feliz", "alegre", "√≥timo", "excelente", "maravilhoso", "perfeito", "sucesso", "bom"]),
    "excited": EmotionConfig("excited", (255, 128, 0), 0.9,
                            ["animado", "empolgado", "incr√≠vel", "fant√°stico", "wow", "uau", "impressionante"]),
    "calm": EmotionConfig("calm", (0, 255, 255), 0.6,
                         ["calmo", "tranquilo", "relaxado", "pac√≠fico", "sereno", "suave"]),
    "thinking": EmotionConfig("thinking", (128, 0, 128), 0.5,
                             ["pensando", "analisando", "considerando", "avaliando", "hmm", "refletindo"]),
    "concerned": EmotionConfig("concerned", (255, 255, 0), 0.7,
                              ["preocupado", "cuidado", "aten√ß√£o", "problema", "erro", "alerta"]),
    "sad": EmotionConfig("sad", (0, 0, 255), 0.4,
                        ["triste", "ruim", "falha", "erro", "problema", "n√£o funcionou", "decepcionado"]),
    "neutral": EmotionConfig("neutral", (128, 128, 128), 0.5,
                            ["neutro", "ok", "normal", "padr√£o", "regular"])
}

class AudioVisualDynamic:
    """Sistema completo √°udio-visual din√¢mico"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.enabled = config.get("enabled", True)
        
        # Conectores validados
        self.elevenlabs_connector = None
        self.audio_player_connector = None
        self.g1_emotion_connector = None
        
        # Estado do sistema
        self.current_emotion = "neutral"
        self.is_playing = False
        self.audio_analysis_thread = None
        self.stop_analysis = False
        
        # Configura√ß√µes de an√°lise de √°udio
        self.volume_smoothing = 0.8  # Suaviza√ß√£o do volume
        self.min_intensity = 0.2     # Intensidade m√≠nima
        self.max_intensity = 1.0     # Intensidade m√°xima
        self.volume_threshold = 0.01 # Limiar de volume m√≠nimo
        
        logger.info("AudioVisualDynamic inicializado")
    
    async def initialize(self) -> bool:
        """Inicializa todos os conectores"""
        try:
            if not self.enabled:
                logger.info("AudioVisualDynamic desabilitado")
                return True
            
            # Importar conectores validados
            from .elevenlabs_tts import ElevenLabsTTSConnector
            from .audio_player import AudioPlayerConnector
            from .g1_emotion_real import G1EmotionRealConnector
            
            # Inicializar ElevenLabs (VALIDADO Teste 5)
            logger.info("Inicializando ElevenLabs TTS...")
            self.elevenlabs_connector = ElevenLabsTTSConnector({
                "enabled": True,
                "output_dir": "audio/speech"
            })
            await self.elevenlabs_connector.initialize()
            
            # Inicializar Audio Player (VALIDADO Teste 1)
            logger.info("Inicializando Audio Player...")
            self.audio_player_connector = AudioPlayerConnector({"enabled": True})
            
            # Inicializar G1 LEDs (VALIDADO Teste 12)
            logger.info("Inicializando G1 LEDs...")
            self.g1_emotion_connector = G1EmotionRealConnector({
                "enabled": True,
                "network_interface": "eth0",
                "timeout": 10.0
            })
            await self.g1_emotion_connector.initialize()
            
            logger.info("‚úÖ AudioVisualDynamic: Todos conectores inicializados")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar AudioVisualDynamic: {e}")
            return False
    
    def detect_emotion_from_text(self, text: str) -> str:
        """Detecta emo√ß√£o a partir do texto"""
        text_lower = text.lower()
        
        # Pontua√ß√£o para cada emo√ß√£o
        emotion_scores = {}
        
        for emotion_name, config in EMOTION_CONFIGS.items():
            score = 0
            for keyword in config.keywords:
                if keyword in text_lower:
                    score += 1
            emotion_scores[emotion_name] = score
        
        # An√°lise de pontua√ß√£o
        if "!" in text:
            emotion_scores["excited"] += 2
        if "?" in text:
            emotion_scores["thinking"] += 1
        if "..." in text:
            emotion_scores["thinking"] += 1
        
        # Retorna emo√ß√£o com maior pontua√ß√£o
        best_emotion = max(emotion_scores, key=emotion_scores.get)
        
        # Se nenhuma emo√ß√£o espec√≠fica, usar neutral
        if emotion_scores[best_emotion] == 0:
            best_emotion = "neutral"
        
        logger.info(f"üé≠ Texto: '{text[:50]}...' ‚Üí Emo√ß√£o: {best_emotion}")
        return best_emotion
    
    def analyze_audio_volume(self, audio_file: str) -> list:
        """Analisa volume do arquivo de √°udio"""
        try:
            with wave.open(audio_file, 'rb') as wav_file:
                frames = wav_file.readframes(-1)
                sound_info = wav_file.getparams()
                
                # Converter para numpy array
                if sound_info.sampwidth == 1:
                    sound_data = np.frombuffer(frames, dtype=np.uint8)
                elif sound_info.sampwidth == 2:
                    sound_data = np.frombuffer(frames, dtype=np.int16)
                elif sound_info.sampwidth == 4:
                    sound_data = np.frombuffer(frames, dtype=np.int32)
                else:
                    logger.warning(f"Formato de √°udio n√£o suportado: {sound_info.sampwidth} bytes")
                    return []
                
                # Calcular RMS em janelas
                window_size = int(sound_info.framerate * 0.1)  # 100ms windows
                volumes = []
                
                for i in range(0, len(sound_data), window_size):
                    window = sound_data[i:i + window_size]
                    if len(window) > 0:
                        rms = np.sqrt(np.mean(window.astype(float) ** 2))
                        # Normalizar para 0-1
                        normalized_volume = min(1.0, rms / 5000.0)
                        volumes.append(normalized_volume)
                
                logger.info(f"üìä √Åudio analisado: {len(volumes)} janelas, volume m√©dio: {np.mean(volumes):.3f}")
                return volumes
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao analisar √°udio: {e}")
            return []
    
    def start_led_animation(self, emotion_config: EmotionConfig, volumes: list):
        """Inicia anima√ß√£o dos LEDs baseada no volume"""
        def animate_leds():
            try:
                logger.info(f"üé® Iniciando anima√ß√£o LED para emo√ß√£o: {emotion_config.name}")
                
                # Timing para sincronizar com √°udio
                window_duration = 0.1  # 100ms por janela
                
                for i, volume in enumerate(volumes):
                    if self.stop_analysis:
                        break
                    
                    # Calcular intensidade baseada no volume
                    volume_intensity = max(self.min_intensity, min(self.max_intensity, volume))
                    final_intensity = emotion_config.base_intensity * volume_intensity
                    
                    # Aplicar intensidade √†s cores RGB
                    r = int(emotion_config.rgb[0] * final_intensity)
                    g = int(emotion_config.rgb[1] * final_intensity)
                    b = int(emotion_config.rgb[2] * final_intensity)
                    
                    # Atualizar LEDs
                    if self.g1_emotion_connector and not self.stop_analysis:
                        try:
                            # Usar asyncio dentro da thread
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                            loop.run_until_complete(
                                self.g1_emotion_connector.set_custom_color(r, g, b)
                            )
                            loop.close()
                        except Exception as led_error:
                            logger.warning(f"Erro ao atualizar LED: {led_error}")
                    
                    logger.debug(f"LED frame {i}: volume={volume:.3f}, intensity={final_intensity:.3f}, RGB({r},{g},{b})")
                    time.sleep(window_duration)
                
                # Desligar LEDs ao final
                if self.g1_emotion_connector and not self.stop_analysis:
                    try:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        loop.run_until_complete(self.g1_emotion_connector.turn_off_leds())
                        loop.close()
                        logger.info("üî¥ LEDs desligados ap√≥s anima√ß√£o")
                    except Exception as e:
                        logger.warning(f"Erro ao desligar LEDs: {e}")
                
            except Exception as e:
                logger.error(f"‚ùå Erro na anima√ß√£o LED: {e}")
        
        self.stop_analysis = False
        self.audio_analysis_thread = threading.Thread(target=animate_leds)
        self.audio_analysis_thread.start()
    
    async def speak_with_dynamic_leds(self, text: str, emotion: Optional[str] = None) -> bool:
        """
        Fala texto com LEDs din√¢micos baseados no volume
        
        Args:
            text: Texto para falar
            emotion: Emo√ß√£o espec√≠fica (ou auto-detecta)
        """
        try:
            # 1. Detectar emo√ß√£o
            if emotion is None:
                emotion = self.detect_emotion_from_text(text)
            
            emotion_config = EMOTION_CONFIGS.get(emotion, EMOTION_CONFIGS["neutral"])
            self.current_emotion = emotion
            
            logger.info(f"üé§ Falando com LEDs din√¢micos: '{text}' (emo√ß√£o: {emotion})")
            
            # 2. Gerar TTS
            if not self.elevenlabs_connector:
                logger.error("‚ùå ElevenLabs n√£o inicializado")
                return False
            
            response = await self.elevenlabs_connector.synthesize_speech(text)
            if not response.success:
                logger.error("‚ùå Falha na gera√ß√£o TTS")
                return False
            
            audio_file = response.file_path
            logger.info(f"üîä TTS gerado: {audio_file}")
            
            # 3. Analisar volume do √°udio
            volumes = self.analyze_audio_volume(audio_file)
            if not volumes:
                logger.warning("‚ö†Ô∏è Falha na an√°lise de volume, usando intensidade fixa")
                volumes = [0.5] * 50  # Fallback com 5 segundos de intensidade m√©dia
            
            # 4. Iniciar anima√ß√£o LED em paralelo
            self.is_playing = True
            self.start_led_animation(emotion_config, volumes)
            
            # 5. Reproduzir √°udio no Anker
            if self.audio_player_connector:
                success = await self.audio_player_connector.play_audio_anker(audio_file)
                if success:
                    logger.info("‚úÖ √Åudio reproduzido no Anker com LEDs din√¢micos")
                    
                    # Aguardar conclus√£o da anima√ß√£o
                    if self.audio_analysis_thread:
                        self.audio_analysis_thread.join(timeout=30)
                    
                    return True
                else:
                    logger.error("‚ùå Falha na reprodu√ß√£o no Anker")
                    self.stop_analysis = True
                    return False
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Erro em speak_with_dynamic_leds: {e}")
            return False
        finally:
            self.is_playing = False
            self.stop_analysis = True
    
    def get_emotion_info(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes das emo√ß√µes dispon√≠veis"""
        return {
            name: {
                "rgb": config.rgb,
                "base_intensity": config.base_intensity,
                "keywords": config.keywords
            }
            for name, config in EMOTION_CONFIGS.items()
        }
