import asyncio
import logging
import os
import requests
import json
from typing import Optional, Dict, Any, List
from pathlib import Path

logger = logging.getLogger(__name__)

class OllamaManager:
    def __init__(self, config_path: str = "config/g1_ollama_llm.json5"):
        self.config_path = Path(config_path)
        self.config = self.load_config()
        self.base_url = f"http://{self.config['llm']['primary']['host']}:{self.config['llm']['primary']['port']}"
        self.current_model = self.config['llm']['primary']['model']
        self.is_initialized = False
        
    def load_config(self) -> Dict[str, Any]:
        """Carrega configura√ß√£o JSON5"""
        try:
            import json5
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json5.load(f)
        except ImportError:
            # Fallback para JSON simples
            with open(self.config_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # Remove coment√°rios simples
                lines = content.split('\n')
                cleaned_lines = []
                for line in lines:
                    if not line.strip().startswith('//'):
                        cleaned_lines.append(line)
                return json.loads('\n'.join(cleaned_lines))
    
    async def initialize(self) -> bool:
        """Inicializa o Ollama Manager"""
        try:
            logger.info("üîß Inicializando Ollama Manager...")
            
            # Verifica se o Ollama est√° rodando
            if not await self._check_ollama_status():
                logger.error("‚ùå Ollama n√£o est√° rodando")
                return False
            
            # Verifica se o modelo principal est√° dispon√≠vel
            if not await self._check_model_availability(self.current_model):
                logger.warning(f"‚ö†Ô∏è Modelo {self.current_model} n√£o encontrado")
                logger.info("üì• Baixando modelo principal...")
                if not await self._download_model(self.current_model):
                    logger.error("‚ùå Falha ao baixar modelo")
                    return False
            
            self.is_initialized = True
            logger.info(f"‚úÖ Ollama Manager inicializado com modelo: {self.current_model}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inicializa√ß√£o: {e}")
            return False
    
    async def _check_ollama_status(self) -> bool:
        """Verifica se o Ollama est√° rodando"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    async def _check_model_availability(self, model_name: str) -> bool:
        """Verifica se um modelo est√° dispon√≠vel"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get('models', [])
                return any(model['name'] == model_name for model in models)
            return False
        except:
            return False
    
    async def _download_model(self, model_name: str) -> bool:
        """Baixa um modelo via Ollama"""
        try:
            logger.info(f"üì• Baixando modelo: {model_name}")
            response = requests.post(
                f"{self.base_url}/api/pull",
                json={"name": model_name},
                stream=True
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        data = json.loads(line)
                        if 'status' in data:
                            logger.info(f"üì• {data['status']}")
                        if data.get('done', False):
                            logger.info(f"‚úÖ Modelo {model_name} baixado com sucesso")
                            return True
            return False
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar modelo: {e}")
            return False
    
    async def generate_response(self, prompt: str, context: str = "", use_local: bool = True) -> Optional[str]:
        """Gera resposta usando Ollama"""
        if not self.is_initialized:
            logger.error("‚ùå Ollama Manager n√£o inicializado")
            return None
        
        try:
            if use_local:
                return await self._generate_ollama(prompt, context)
            else:
                return await self._generate_openai(prompt, context)
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o: {e}")
            return None
    
    async def _generate_ollama(self, prompt: str, context: str = "") -> Optional[str]:
        """Gera resposta usando Ollama"""
        try:
            # Obt√©m configura√ß√µes
            config = self.config['llm']['primary']
            response_style = config.get("response_style", "normal")
            
            # Ajusta prompt baseado no estilo de resposta
            if response_style == "concise":
                style_instruction = "Resposta curta e direta, m√°ximo 2-3 frases."
            else:
                style_instruction = ""
            
            full_prompt = f"{context}\n\n{style_instruction}\n\n{prompt}" if context else f"{style_instruction}\n\n{prompt}"
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.current_model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": config["temperature"],
                        "top_p": config["top_p"],
                        "num_predict": config["max_tokens"]
                    }
                },
                timeout=config["timeout"]
            )
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get('response', '').strip()
                
                # Limita resposta se necess√°rio
                max_tokens = config["max_tokens"]
                if len(response_text.split()) > max_tokens // 4:  # Aproximadamente max_tokens/4 palavras
                    words = response_text.split()[:max_tokens // 4]
                    response_text = " ".join(words) + "..."
                
                return response_text
            else:
                error_msg = response.text
                if response.status_code == 404:
                    logger.error("‚ùå OLLAMA: Modelo n√£o encontrado!")
                    logger.error(f"   ü§ñ Modelo: {self.current_model}")
                    logger.error("   üì• Execute: ollama pull llama3.1:8b")
                elif response.status_code == 500:
                    logger.error("‚ùå OLLAMA: Erro interno do servidor!")
                    logger.error("   üîÑ Reinicie o Ollama: brew services restart ollama")
                elif "timeout" in error_msg.lower():
                    logger.error("‚ùå OLLAMA: Timeout na resposta!")
                    logger.error("   ‚è∞ Aumente o timeout ou use um modelo menor")
                else:
                    logger.error(f"‚ùå OLLAMA: Erro {response.status_code}")
                    logger.error(f"   üìù Detalhes: {error_msg}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o Ollama: {e}")
            return None
    
    async def _generate_openai(self, prompt: str, context: str = "") -> Optional[str]:
        """Gera resposta usando OpenAI (fallback)"""
        try:
            import openai
            
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                logger.error("‚ùå OpenAI API key n√£o configurada")
                return None
            
            # Usa a nova API do OpenAI
            client = openai.OpenAI(api_key=api_key)
            
            # Adiciona instru√ß√£o para resposta curta
            concise_instruction = "Resposta curta e direta, m√°ximo 2-3 frases."
            full_prompt = f"{context}\n\n{concise_instruction}\n\n{prompt}" if context else f"{concise_instruction}\n\n{prompt}"
            
            response = await client.chat.completions.acreate(
                model=self.config['llm']['fallback']['model'],
                messages=[{"role": "user", "content": full_prompt}],
                temperature=self.config['llm']['fallback']['temperature'],
                max_tokens=self.config['llm']['fallback']['max_tokens']
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            error_msg = str(e)
            if "quota_exceeded" in error_msg or "billing" in error_msg:
                logger.error("‚ùå OPENAI: Cota excedida!")
                logger.error("   üí≥ Adicione cr√©ditos em: https://platform.openai.com/billing")
                logger.error("   üìä Verifique seu uso em: https://platform.openai.com/usage")
            elif "invalid_api_key" in error_msg or "authentication" in error_msg:
                logger.error("‚ùå OPENAI: API Key inv√°lida!")
                logger.error("   üîë Verifique OPENAI_API_KEY no arquivo .env")
                logger.error("   üîó Obtenha uma nova key em: https://platform.openai.com/api-keys")
            elif "rate_limit" in error_msg:
                logger.error("‚ùå OPENAI: Limite de requisi√ß√µes excedido!")
                logger.error("   ‚è∞ Aguarde um momento e tente novamente")
            else:
                logger.error(f"‚ùå OPENAI: Erro na gera√ß√£o - {error_msg}")
            return None
    
    async def switch_model(self, model_name: str) -> bool:
        """Troca para outro modelo"""
        try:
            logger.info(f"üîÑ Trocando para modelo: {model_name}")
            
            # Verifica se o modelo existe
            if not await self._check_model_availability(model_name):
                logger.info(f"üì• Baixando modelo: {model_name}")
                if not await self._download_model(model_name):
                    return False
            
            self.current_model = model_name
            logger.info(f"‚úÖ Modelo trocado para: {model_name}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao trocar modelo: {e}")
            return False
    
    async def list_models(self) -> List[Dict[str, Any]]:
        """Lista modelos dispon√≠veis"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                return response.json().get('models', [])
            return []
        except:
            return []
    
    async def test_performance(self) -> Dict[str, Any]:
        """Testa performance dos modelos"""
        test_prompt = "Ol√°, como voc√™ est√°? Responda em portugu√™s."
        
        results = {
            "ollama": {"success": False, "time": 0, "response": ""},
            "openai": {"success": False, "time": 0, "response": ""}
        }
        
        # Teste Ollama
        try:
            start_time = asyncio.get_event_loop().time()
            response = await self._generate_ollama(test_prompt)
            end_time = asyncio.get_event_loop().time()
            
            results["ollama"] = {
                "success": response is not None,
                "time": end_time - start_time,
                "response": response or ""
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no teste Ollama: {e}")
        
        # Teste OpenAI
        try:
            start_time = asyncio.get_event_loop().time()
            response = await self._generate_openai(test_prompt)
            end_time = asyncio.get_event_loop().time()
            
            results["openai"] = {
                "success": response is not None,
                "time": end_time - start_time,
                "response": response or ""
            }
        except Exception as e:
            logger.error(f"‚ùå Erro no teste OpenAI: {e}")
        
        return results
    
    async def cleanup(self):
        """Limpa recursos"""
        logger.info("üßπ Limpando Ollama Manager...")
        self.is_initialized = False
        logger.info("‚úÖ Ollama Manager limpo")
