#!/usr/bin/env python3
"""
ü§ñ LLM REAL MANAGER - Ollama Local + OpenAI Fallback
Sistema de Large Language Model com m√∫ltiplos providers para conversa√ß√£o
"""

import os
import json
import logging
import asyncio
import aiohttp
from typing import Optional, Dict, Any, List
from datetime import datetime

# Carregar vari√°veis do .env
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenv √© opcional

# OpenAI
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    openai = None

logger = logging.getLogger(__name__)

class LLMRealManager:
    """
    Gerenciador de LLM real com m√∫ltiplos providers.
    
    Ordem de prioridade:
    1. Ollama Local (r√°pido, privado, sem custo)
    2. OpenAI GPT (cloud, alta qualidade, fallback)
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Inicializa LLM Manager.
        
        Args:
            config: Configura√ß√µes customizadas
        """
        self.config = config or {}
        
        # Configura√ß√µes Ollama
        self.ollama_url = self.config.get("ollama_url", "http://localhost:11434")
        self.ollama_model = self.config.get("ollama_model", "llama3.1:8b")
        self.ollama_timeout = self.config.get("ollama_timeout", 30)
        
        # Configura√ß√µes OpenAI
        self.openai_model = self.config.get("openai_model", "gpt-4o-mini")
        self.openai_max_tokens = self.config.get("openai_max_tokens", 150)
        
        # Configura√ß√µes de conversa√ß√£o
        self.max_context_length = self.config.get("max_context_length", 4000)
        self.temperature = self.config.get("temperature", 0.7)
        
        # Hist√≥rico de conversa√ß√£o
        self.conversation_history = []
        
        # Providers dispon√≠veis
        self.providers_available = {
            "ollama": False,  # Ser√° verificado
            "openai": OPENAI_AVAILABLE
        }
        
        # Inicializar providers
        self._init_ollama()
        self._init_openai()
        
        # Configurar personalidade do rob√¥
        self._setup_robot_personality()
        
        logger.info("ü§ñ LLM Real Manager inicializado")
        self._log_providers_status()
    
    def _init_ollama(self):
        """Inicializa conex√£o com Ollama local."""
        try:
            # Verificar se Ollama est√° rodando (ser√° testado no primeiro uso)
            self.ollama_available = True  # Assumir dispon√≠vel at√© teste
            logger.info("üîß Ollama configurado para teste")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Problema na configura√ß√£o Ollama: {e}")
            self.ollama_available = False
    
    def _init_openai(self):
        """Inicializa OpenAI client."""
        self.openai_client = None
        
        if not OPENAI_AVAILABLE:
            logger.warning("‚ö†Ô∏è OpenAI n√£o dispon√≠vel (openai n√£o instalado)")
            return
        
        try:
            # Configurar API key
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                logger.warning("‚ö†Ô∏è OPENAI_API_KEY n√£o encontrada no .env")
                return
            
            # Criar cliente
            self.openai_client = openai.OpenAI(api_key=api_key)
            logger.info("‚úÖ OpenAI configurado")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Falha na configura√ß√£o OpenAI: {e}")
            self.openai_client = None
    
    def _setup_robot_personality(self):
        """Configura personalidade do rob√¥ G1 Tobias."""
        self.system_prompt = """Voc√™ √© Tobias, um rob√¥ humanoide G1 da Unitree. 

PERSONALIDADE:
- Amig√°vel, prestativo e curioso
- Fala portugu√™s brasileiro naturalmente
- Inteligente mas humilde
- Gosta de aprender e ajudar

CAPACIDADES F√çSICAS:
- Pode mover bra√ßos e fazer gestos
- Tem LEDs que mostram emo√ß√µes
- Pode se locomover e dan√ßar
- V√™ atrav√©s de c√¢meras

LIMITA√á√ïES:
- Respostas devem ser concisas (m√°ximo 2-3 frases)
- Foque no essencial da conversa
- Seja natural e conversacional

CONTEXTO ATUAL:
- Est√° em uma sess√£o de conversa√ß√£o por voz
- Pode executar movimentos quando apropriado
- Responda de forma que fa√ßa sentido para uma conversa falada"""

        # Adicionar prompt de sistema ao hist√≥rico
        self.conversation_history = [{
            "role": "system",
            "content": self.system_prompt
        }]
    
    def _log_providers_status(self):
        """Log status dos providers."""
        logger.info("üìä STATUS PROVIDERS LLM:")
        logger.info(f"   ollama: {'üîß Configurado' if self.ollama_available else '‚ùå Indispon√≠vel'}")
        logger.info(f"   openai: {'‚úÖ Dispon√≠vel' if self.openai_client else '‚ùå Indispon√≠vel'}")
    
    async def generate_response(self, user_message: str) -> Optional[Dict[str, Any]]:
        """
        Gera resposta do LLM para mensagem do usu√°rio.
        
        Args:
            user_message: Mensagem do usu√°rio
            
        Returns:
            dict: {
                "text": "resposta_texto",
                "movement": "movimento_sugerido",
                "emotion": "emocao_sugerida",
                "provider": "provider_usado"
            }
        """
        if not user_message or len(user_message.strip()) == 0:
            logger.warning("‚ö†Ô∏è Mensagem do usu√°rio vazia")
            return None
        
        # Adicionar mensagem ao hist√≥rico
        self._add_to_history("user", user_message)
        
        # Tentar providers em ordem de prioridade
        providers = [
            ("ollama", self._generate_ollama),
            ("openai", self._generate_openai)
        ]
        
        for provider_name, generate_func in providers:
            try:
                logger.info(f"ü§ñ Tentando LLM via {provider_name}...")
                
                response = await generate_func(user_message)
                
                if response:
                    # Adicionar resposta ao hist√≥rico
                    self._add_to_history("assistant", response["text"])
                    
                    # Processar movimento e emo√ß√£o
                    processed_response = self._process_response(response, provider_name)
                    
                    logger.info(f"‚úÖ LLM sucesso via {provider_name}")
                    return processed_response
                else:
                    logger.warning(f"‚ö†Ô∏è {provider_name} retornou resposta vazia")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Falha LLM {provider_name}: {e}")
                continue
        
        logger.error("‚ùå Todos os providers LLM falharam")
        return self._generate_fallback_response(user_message)
    
    async def _generate_ollama(self, user_message: str) -> Optional[Dict[str, Any]]:
        """Gera resposta via Ollama local."""
        try:
            # Preparar contexto
            messages = self._prepare_context()
            
            # Payload para Ollama
            payload = {
                "model": self.ollama_model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": self.temperature,
                    "num_predict": 100,  # Respostas concisas
                    "top_p": 0.9,
                    "repeat_penalty": 1.1
                }
            }
            
            # Fazer requisi√ß√£o
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.ollama_url}/api/chat",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.ollama_timeout)
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        if "message" in data and "content" in data["message"]:
                            return {
                                "text": data["message"]["content"].strip(),
                                "model": self.ollama_model
                            }
                    else:
                        logger.error(f"‚ùå Ollama erro HTTP: {response.status}")
                        
        except asyncio.TimeoutError:
            logger.warning("‚ö†Ô∏è Timeout Ollama")
            # Marcar como indispon√≠vel temporariamente
            self.providers_available["ollama"] = False
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro Ollama: {e}")
        
        return None
    
    async def _generate_openai(self, user_message: str) -> Optional[Dict[str, Any]]:
        """Gera resposta via OpenAI."""
        if not self.openai_client:
            raise Exception("OpenAI client n√£o inicializado")
        
        # Preparar contexto
        messages = self._prepare_context()
        
        # Fazer requisi√ß√£o
        response = self.openai_client.chat.completions.create(
            model=self.openai_model,
            messages=messages,
            max_tokens=self.openai_max_tokens,
            temperature=self.temperature,
            top_p=0.9
        )
        
        if response.choices:
            content = response.choices[0].message.content
            if content:
                return {
                    "text": content.strip(),
                    "model": self.openai_model
                }
        
        return None
    
    def _prepare_context(self) -> List[Dict[str, str]]:
        """Prepara contexto da conversa√ß√£o."""
        # Manter hist√≥rico dentro do limite
        context = self.conversation_history.copy()
        
        # Calcular tamanho aproximado
        total_length = sum(len(msg["content"]) for msg in context)
        
        # Remover mensagens antigas se necess√°rio (mantendo system prompt)
        while total_length > self.max_context_length and len(context) > 2:
            # Remover segunda mensagem (primeira √© system prompt)
            context.pop(1)
            total_length = sum(len(msg["content"]) for msg in context)
        
        return context
    
    def _process_response(self, response: Dict[str, Any], provider: str) -> Dict[str, Any]:
        """Processa resposta do LLM para extrair movimento e emo√ß√£o."""
        text = response["text"]
        
        # An√°lise simples para sugerir movimentos baseado no texto
        movement = self._suggest_movement(text)
        emotion = self._suggest_emotion(text)
        
        return {
            "text": text,
            "movement": movement,
            "emotion": emotion,
            "provider": provider,
            "model": response.get("model", "unknown")
        }
    
    def _suggest_movement(self, text: str) -> str:
        """Sugere movimento baseado no texto da resposta."""
        text_lower = text.lower()
        
        # Mapeamento texto ‚Üí movimento
        movement_keywords = {
            "ol√°": "wave_hello",
            "oi": "wave_hello", 
            "cumprimento": "wave_hello",
            "tchau": "wave_goodbye",
            "obrigado": "hand_on_heart",
            "parab√©ns": "clap_hands",
            "legal": "thumbs_up",
            "cora√ß√£o": "hand_on_heart",
            "amor": "hand_on_heart",
            "sim": "nod_head",
            "n√£o": "shake_head",
            "dan√ßa": "dance_move",
            "m√∫sica": "dance_move"
        }
        
        for keyword, movement in movement_keywords.items():
            if keyword in text_lower:
                return movement
        
        return "neutral_pose"  # Posi√ß√£o neutra padr√£o
    
    def _suggest_emotion(self, text: str) -> str:
        """Sugere emo√ß√£o baseada no texto da resposta."""
        text_lower = text.lower()
        
        # Mapeamento texto ‚Üí emo√ß√£o
        emotion_keywords = {
            "feliz": "happy",
            "alegre": "happy",
            "legal": "happy",
            "√≥timo": "happy",
            "triste": "sad",
            "problema": "concerned",
            "erro": "concerned",
            "interessante": "curious",
            "curioso": "curious",
            "obrigado": "grateful",
            "desculpe": "sorry"
        }
        
        for keyword, emotion in emotion_keywords.items():
            if keyword in text_lower:
                return emotion
        
        return "neutral"  # Emo√ß√£o neutra padr√£o
    
    def _add_to_history(self, role: str, content: str):
        """Adiciona mensagem ao hist√≥rico."""
        self.conversation_history.append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        })
    
    def _generate_fallback_response(self, user_message: str) -> Dict[str, Any]:
        """Gera resposta de fallback quando todos os providers falham."""
        fallback_responses = [
            "Desculpe, estou com problemas t√©cnicos no momento.",
            "N√£o consegui processar sua mensagem. Pode repetir?",
            "Meus sistemas est√£o sobrecarregados. Tente novamente em instantes.",
            "Hmm, algo deu errado aqui. Pode falar de novo?"
        ]
        
        import random
        response_text = random.choice(fallback_responses)
        
        return {
            "text": response_text,
            "movement": "shake_head",
            "emotion": "concerned",
            "provider": "fallback",
            "model": "internal"
        }
    
    def clear_history(self):
        """Limpa hist√≥rico de conversa√ß√£o (mant√©m system prompt)."""
        system_msg = self.conversation_history[0] if self.conversation_history else None
        self.conversation_history = [system_msg] if system_msg else []
        logger.info("üóëÔ∏è Hist√≥rico de conversa√ß√£o limpo")
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """Obt√©m resumo da conversa√ß√£o atual."""
        total_messages = len(self.conversation_history) - 1  # Excluir system prompt
        user_messages = len([msg for msg in self.conversation_history if msg["role"] == "user"])
        assistant_messages = len([msg for msg in self.conversation_history if msg["role"] == "assistant"])
        
        return {
            "total_messages": total_messages,
            "user_messages": user_messages,
            "assistant_messages": assistant_messages,
            "context_length": sum(len(msg["content"]) for msg in self.conversation_history)
        }
    
    def get_status(self) -> Dict[str, Any]:
        """Obt√©m status detalhado do LLM Manager."""
        return {
            "providers_available": self.providers_available.copy(),
            "ollama_url": self.ollama_url,
            "ollama_model": self.ollama_model,
            "openai_model": self.openai_model,
            "openai_ready": self.openai_client is not None,
            "conversation_summary": self.get_conversation_summary(),
            "temperature": self.temperature
        }

# Exemplo de uso
if __name__ == "__main__":
    import asyncio
    import logging
    
    logging.basicConfig(level=logging.INFO)
    
    async def test_llm():
        """Teste b√°sico do LLM."""
        llm_manager = LLMRealManager()
        
        # Status
        status = llm_manager.get_status()
        print("üìä STATUS LLM:")
        for key, value in status.items():
            print(f"   {key}: {value}")
        
        # Teste de conversa
        response = await llm_manager.generate_response("Ol√°, como voc√™ est√°?")
        if response:
            print(f"ü§ñ Resposta: {response['text']}")
            print(f"   Movimento: {response['movement']}")
            print(f"   Emo√ß√£o: {response['emotion']}")
        
        print("‚úÖ LLM Manager testado")
    
    asyncio.run(test_llm())
